import streamlit as st
import pandas as pd
import asyncio
from src.logic.llm import run_batch_annotation
from src.logic.schema import convert_ui_fields_to_schema
from src.logic.metrics import calculate_metrics

def render_eval_tab(config):
    st.header("è¯„ä¼°ä¸ä¿¡åº¦ (Evaluation & Reliability)")
    
    if st.session_state.df is None:
        st.warning("è¯·å…ˆä¸Šä¼ æ•°æ®ã€‚")
        return

    # Initialize Validation State
    if 'validation_indices' not in st.session_state:
        st.session_state.validation_indices = []
    if 'human_annotations' not in st.session_state:
        st.session_state.human_annotations = {} # {index: {field: value}}
    
    # Store experiment results in session state to persist between reruns
    if 'experiment_results' not in st.session_state:
        st.session_state.experiment_results = {} # {config_name: {index: result_dict}}

    tab_set, tab_run = st.tabs(["1. æ„å»ºéªŒè¯é›† (Human Label)", "2. æç¤ºè¯å¯¹æ¯”å®éªŒ (Experiments)"])
    
    # --- Tab 1: Build Validation Set ---
    with tab_set:
        st.subheader("äººå·¥æ ‡æ³¨éªŒè¯é›†")
        
        # Select Sample
        c1, c2 = st.columns([1, 1])
        with c1:
            n_val = st.number_input("æ·»åŠ éšæœºæ ·æœ¬æ•°é‡", 1, 100, 10, key="n_val_input")
        with c2:
            st.write("") # Spacer
            st.write("")
            if st.button("â• æ·»åŠ æ ·æœ¬åˆ°éªŒè¯é›†"):
                current_indices = set(st.session_state.validation_indices)
                available_indices = [i for i in st.session_state.df.index if i not in current_indices]
                
                if len(available_indices) < n_val:
                    st.warning("å‰©ä½™å¯ç”¨æ•°æ®ä¸è¶³ã€‚")
                    to_add = available_indices
                else:
                    import random
                    to_add = random.sample(available_indices, n_val)
                
                st.session_state.validation_indices.extend(to_add)
                st.success(f"å·²æ·»åŠ  {len(to_add)} æ¡æ•°æ®ã€‚")
                st.rerun()
            
        if not st.session_state.validation_indices:
            st.info("éªŒè¯é›†ä¸ºç©ºã€‚è¯·å…ˆæ·»åŠ æ ·æœ¬ã€‚")
        else:
            st.markdown(f"**éªŒè¯é›†å¤§å°:** {len(st.session_state.validation_indices)}")
            
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºéªŒè¯é›†", type="secondary"):
                st.session_state.validation_indices = []
                st.session_state.human_annotations = {}
                st.rerun()
            
            st.divider()
            
            # --- New Feature: Import Ground Truth ---
            with st.expander("ğŸ“‚ ä»æ•°æ®åˆ—å¯¼å…¥æ­£ç¡®ç­”æ¡ˆ (Import Ground Truth)", expanded=False):
                st.info("å¦‚æœæ‚¨ä¸Šä¼ çš„æ•°æ®ä¸­å·²ç»åŒ…å«äº†æŸäº›å˜é‡çš„æ­£ç¡®æ ‡æ³¨ï¼ˆGround Truthï¼‰ï¼Œå¯ä»¥åœ¨æ­¤å°†å…¶æ‰¹é‡æ˜ å°„åˆ°éªŒè¯é›†ï¼Œæ— éœ€æ‰‹åŠ¨é‡æ–°æ ‡æ³¨ã€‚")
                
                mapping = {}
                cols = st.session_state.df.columns.tolist()
                cols_options = ["(ä¸å¯¼å…¥)"] + cols
                
                # Grid layout for mapping
                m_cols = st.columns(3)
                
                for i, field in enumerate(st.session_state.schema_fields):
                    fname = field["name"]
                    if not fname: continue
                    
                    # Auto-match if column name matches field name
                    default_idx = 0
                    if fname in cols:
                        default_idx = cols_options.index(fname)
                    
                    with m_cols[i % 3]:
                        mapping[fname] = st.selectbox(
                            f"Field `{fname}` å¯¹åº”åˆ—:", 
                            cols_options, 
                            index=default_idx,
                            key=f"map_{fname}"
                        )
                
                if st.button("ğŸ“¥ å¼€å§‹å¯¼å…¥ (Import)", type="primary"):
                    imported_count = 0
                    for idx in st.session_state.validation_indices:
                        if idx not in st.session_state.human_annotations:
                            st.session_state.human_annotations[idx] = {}
                            
                        for fname, col_name in mapping.items():
                            if col_name != "(ä¸å¯¼å…¥)":
                                val = st.session_state.df.at[idx, col_name]
                                st.session_state.human_annotations[idx][fname] = str(val)
                    
                    st.success(f"æˆåŠŸä¸ºéªŒè¯é›†å¯¼å…¥äº†æ ‡æ³¨æ•°æ®ï¼")
                    st.rerun()

            st.divider()

            # --- Annotation Interface ---
            
            # Select Text Column to Display
            cols = st.session_state.df.columns.tolist()
            # Try to guess text column (contains 'text', 'content', 'body' or is object type)
            default_idx = 0
            for i, col in enumerate(cols):
                if any(x in col.lower() for x in ['text', 'content', 'body', 'comment', 'review']):
                    default_idx = i
                    break
            
            display_col = st.selectbox("é€‰æ‹©ç”¨äºæ ‡æ³¨å‚è€ƒçš„æ–‡æœ¬åˆ—:", cols, index=default_idx)
            
            st.divider()

            val_df = st.session_state.df.loc[st.session_state.validation_indices]
            schema_fields = st.session_state.schema_fields
            
            for idx, row in val_df.iterrows():
                # Card-like container
                with st.container(border=True):
                    # Display Text Content
                    text_content = row[display_col]
                    st.markdown(f"**{display_col}:**")
                    st.info(f"{text_content}") # Use st.info box for better readability of text
                    st.caption(f"Row Index: {idx}")
                    
                    # Ensure storage exists
                    if idx not in st.session_state.human_annotations:
                        st.session_state.human_annotations[idx] = {}
                    
                    # Input fields grid
                    input_cols = st.columns(len(schema_fields))
                    
                    for i, field in enumerate(schema_fields):
                        field_name = field["name"]
                        if not field_name: continue
                        
                        current_val = st.session_state.human_annotations[idx].get(field_name, None)
                        
                        with input_cols[i]:
                            if field["type"] == "Enum":
                                options = [opt.strip() for opt in field["options"].split(",") if opt.strip()]
                                index = options.index(current_val) if current_val in options else 0
                                
                                new_val = st.selectbox(
                                    f"{field_name}", 
                                    options, 
                                    index=index,
                                    key=f"human_{idx}_{field_name}",
                                    label_visibility="visible"
                                )
                                st.session_state.human_annotations[idx][field_name] = new_val
                            else:
                                new_val = st.text_input(
                                    f"{field_name}",
                                    value=str(current_val) if current_val else "",
                                    key=f"human_{idx}_{field_name}"
                                )
                                st.session_state.human_annotations[idx][field_name] = new_val

    # --- Tab 2: Run Experiments ---
    with tab_run:
        st.subheader("æç¤ºè¯ç»„åˆå¯¹æ¯”å®éªŒ")
        
        if not st.session_state.validation_indices:
            st.warning("âš ï¸ è¯·å…ˆåœ¨ Tab 1 æ„å»ºéªŒè¯é›†å¹¶è¿›è¡Œäººå·¥æ ‡æ³¨ã€‚")
            return
        elif not config["api_key"]:
            st.error("âš ï¸ è¯·é…ç½® API Keyã€‚")
            return
            
        # 1. Select Configs to Run
        all_configs = st.session_state.prompt_configs
        config_names = [c["name"] for c in all_configs]
        
        selected_configs = st.multiselect(
            "é€‰æ‹©è¦å¯¹æ¯”çš„æç¤ºè¯é…ç½® (Select Configs)", 
            config_names, 
            default=[all_configs[st.session_state.current_config_idx]["name"]]
        )
        
        if st.button("ğŸš€ è¿è¡Œå®éªŒ (Run Experiments)", type="primary"):
            val_df = st.session_state.df.loc[st.session_state.validation_indices].copy()
            schema = convert_ui_fields_to_schema(st.session_state.schema_fields)
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            total_steps = len(selected_configs)
            
            for i, cfg_name in enumerate(selected_configs):
                status_text.text(f"æ­£åœ¨è¿è¡Œé…ç½®: {cfg_name} ({i+1}/{total_steps})...")
                
                # Find config object
                cfg_obj = next(c for c in all_configs if c["name"] == cfg_name)
                
                try:
                    # Run Batch
                    results = asyncio.run(run_batch_annotation(
                        val_df,
                        cfg_obj["system"],
                        cfg_obj["user"],
                        schema,
                        config
                    ))
                    
                    # Store processed results
                    results_map = {res["index"]: res for res in results}
                    st.session_state.experiment_results[cfg_name] = results_map
                    
                except Exception as e:
                    st.error(f"é…ç½® {cfg_name} è¿è¡Œå¤±è´¥: {e}")
                
                progress_bar.progress((i + 1) / total_steps)
            
            status_text.success("æ‰€æœ‰é…ç½®è¿è¡Œå®Œæˆï¼")

        st.divider()

        # 2. Display Results & Analysis
        if selected_configs and any(name in st.session_state.experiment_results for name in selected_configs):
            st.subheader("ğŸ“Š å®éªŒç»“æœåˆ†æ")
            
            # Prepare Analysis Data
            # We want a DataFrame where:
            # Index: Row ID
            # Columns: Human_{Field}, ConfigA_{Field}, ConfigB_{Field}...
            
            analysis_rows = []
            
            for idx in st.session_state.validation_indices:
                row_data = {"index": idx}
                
                # Human Labels
                h_labels = st.session_state.human_annotations.get(idx, {})
                for k, v in h_labels.items():
                    row_data[f"Human_{k}"] = v
                
                # AI Labels for each Config
                for cfg_name in selected_configs:
                    if cfg_name in st.session_state.experiment_results:
                        res_map = st.session_state.experiment_results[cfg_name]
                        ai_res = res_map.get(idx, {}).get("parsed", {})
                        for k, v in ai_res.items():
                            row_data[f"{cfg_name}_{k}"] = v
                
                analysis_rows.append(row_data)
            
            df_analysis = pd.DataFrame(analysis_rows)
            st.dataframe(df_analysis, use_container_width=True)
            
            # 3. Metrics Table
            st.subheader("ğŸ“ˆ ä¿¡æ•ˆåº¦æŒ‡æ ‡ (Metrics)")
            
            # For each field, compare Configs vs Human
            for field in st.session_state.schema_fields:
                fname = field["name"]
                if not fname: continue
                
                with st.expander(f"å­—æ®µ: {fname}", expanded=True):
                    metrics_data = []
                    
                    # Human Column
                    h_col = f"Human_{fname}"
                    if h_col not in df_analysis.columns:
                        st.warning(f"å­—æ®µ {fname} ç¼ºå°‘äººå·¥æ ‡æ³¨ã€‚")
                        continue
                        
                    # Calculate metrics for each config
                    for cfg_name in selected_configs:
                        a_col = f"{cfg_name}_{fname}"
                        if a_col in df_analysis.columns:
                            m = calculate_metrics(df_analysis, h_col, a_col)
                            metrics_data.append({
                                "Configuration": cfg_name,
                                "Accuracy": f"{m['accuracy']:.2%}",
                                "Kappa": f"{m['kappa']:.4f}",
                                "N": m["n"]
                            })
                    
                    if metrics_data:
                        st.table(pd.DataFrame(metrics_data).set_index("Configuration"))
                    else:
                        st.info("æ— æœ‰æ•ˆæ•°æ®è®¡ç®—æŒ‡æ ‡ã€‚")

            # 4. Inter-Config Comparison (Optional)
            if len(selected_configs) > 1:
                st.subheader("ğŸ¤ é…ç½®é—´ä¸€è‡´æ€§ (Inter-Config Agreement)")
                st.caption("æ¯”è¾ƒä¸åŒæç¤ºè¯é…ç½®ä¹‹é—´çš„è¾“å‡ºä¸€è‡´æ€§ (Cohen's Kappa)")
                
                # Matrix
                for field in st.session_state.schema_fields:
                    fname = field["name"]
                    if not fname: continue
                    
                    st.markdown(f"**å­—æ®µ: {fname}**")
                    matrix = pd.DataFrame(index=selected_configs, columns=selected_configs)
                    
                    for c1 in selected_configs:
                        for c2 in selected_configs:
                            col1 = f"{c1}_{fname}"
                            col2 = f"{c2}_{fname}"
                            if col1 in df_analysis.columns and col2 in df_analysis.columns:
                                m = calculate_metrics(df_analysis, col1, col2)
                                matrix.loc[c1, c2] = f"{m['kappa']:.4f}"
                    
                    st.table(matrix)