import streamlit as st
import pandas as pd
import asyncio
import math
import time
from src.logic.llm import run_batch_annotation
from src.logic.schema import convert_ui_fields_to_schema
from src.logic.generator import generate_python_script

def render_playground_tab(config):
    st.header("æ ‡æ³¨æ‰§è¡Œå° (Annotation Runner)")
    
    if st.session_state.df is None:
        st.warning("è¯·å…ˆåœ¨â€œæ•°æ®ä¸Šä¼ â€æ ‡ç­¾é¡µä¸Šä¼ æ•°æ®ã€‚")
        return
    
    # --- 1. Mode Selection ---
    st.subheader("1. é€‰æ‹©è¿è¡Œæ¨¡å¼")
    
    mode = st.radio("æ¨¡å¼", ["è°ƒè¯•æ¨¡å¼ (Debug / Sample)", "ç”Ÿäº§æ¨¡å¼ (Full Batch)"], horizontal=True)
    
    target_df = None
    
    if mode == "è°ƒè¯•æ¨¡å¼ (Debug / Sample)":
        st.info("åœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œä»…æŠ½å–å°‘é‡æ•°æ®è¿›è¡Œæµ‹è¯•ï¼Œç”¨äºéªŒè¯ Prompt å’Œ Schema æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚")
        
        c1, c2 = st.columns([1, 2])
        with c1:
            sample_method = st.selectbox("é‡‡æ ·æ–¹å¼", ["å‰ N è¡Œ", "éšæœºé‡‡æ ·", "å…³é”®è¯è¿‡æ»¤"])
        
        with c2:
            if sample_method == "å‰ N è¡Œ":
                n = st.number_input("è¡Œæ•°", 1, 100, 5)
                target_df = st.session_state.df.head(n).copy()
                
            elif sample_method == "éšæœºé‡‡æ ·":
                n = st.number_input("è¡Œæ•°", 1, 100, 5)
                if len(st.session_state.df) > 0:
                    target_df = st.session_state.df.sample(min(n, len(st.session_state.df))).copy()
                else:
                    target_df = pd.DataFrame()
                    
            elif sample_method == "å…³é”®è¯è¿‡æ»¤":
                col = st.selectbox("ç­›é€‰åˆ—", st.session_state.df.columns)
                keyword = st.text_input("åŒ…å«å…³é”®è¯")
                n = st.number_input("æœ€å¤§è¿”å›è¡Œæ•°", 1, 100, 5)
                
                if keyword:
                    mask = st.session_state.df[col].astype(str).str.contains(keyword, case=False, na=False)
                    filtered = st.session_state.df[mask]
                    target_df = filtered.head(n).copy()
                else:
                    target_df = st.session_state.df.head(n).copy()
        
        st.markdown(f"**å½“å‰é¢„è§ˆ (Preview): {len(target_df)} rows**")
        st.dataframe(target_df.head(), width="stretch")

    else: # Production Mode
        st.warning("âš ï¸ ç”Ÿäº§æ¨¡å¼å°†å¯¹**æ‰€æœ‰**ä¸Šä¼ æ•°æ®è¿›è¡Œæ ‡æ³¨ã€‚è¯·ç¡®ä¿æ‚¨çš„ API Key ä½™é¢å……è¶³ï¼Œå¹¶ä¸”å·²åœ¨è°ƒè¯•æ¨¡å¼ä¸‹éªŒè¯è¿‡æ•ˆæœã€‚")
        target_df = st.session_state.df.copy()
        st.markdown(f"**å¾…å¤„ç†æ•°æ®æ€»é‡: {len(target_df)} rows**")
        
        # Cost Estimation (Rough)
        avg_tokens = 500 # Assumption
        total_est_tokens = len(target_df) * avg_tokens
        st.caption(f"é¢„è®¡æ¶ˆè€— Token (ä¼°ç®—): ~{total_est_tokens/1000:.1f}k (ä»…ä¾›å‚è€ƒï¼Œå–å†³äºæ–‡æœ¬é•¿åº¦)")

    # --- 2. Run Annotation ---
    st.markdown("---")
    st.subheader("2. æ‰§è¡Œæ ‡æ³¨")
    
    if not config["api_key"]:
        st.error("è¯·åœ¨å·¦ä¾§æ è¾“å…¥ DeepSeek API Keyã€‚")
        return

    run_btn = st.button("ğŸš€ å¼€å§‹è¿è¡Œä»»åŠ¡", type="primary")
    
    if run_btn:
        schema = convert_ui_fields_to_schema(st.session_state.schema_fields)
        
        # Progress UI
        progress_bar = st.progress(0)
        status_text = st.empty()
        metrics_col1, metrics_col2 = st.columns(2)
        
        # Calculate Batches
        batch_size = config.get("batch_size", 1)
        total_rows = len(target_df)
        total_batches = math.ceil(total_rows / batch_size)
        completed_batches = 0
        
        start_time = time.time()
        
        def update_progress():
            nonlocal completed_batches
            completed_batches += 1
            progress = min(completed_batches / total_batches, 1.0)
            progress_bar.progress(progress)
            
            elapsed = time.time() - start_time
            avg_time_per_batch = elapsed / completed_batches if completed_batches > 0 else 0
            remaining_batches = total_batches - completed_batches
            est_remaining = remaining_batches * avg_time_per_batch
            
            status_text.markdown(f"**è¿›åº¦:** {completed_batches}/{total_batches} æ‰¹æ¬¡ | **é¢„è®¡å‰©ä½™æ—¶é—´:** {est_remaining:.1f}s")

        try:
            with st.spinner("æ­£åœ¨è¿æ¥ DeepSeek API è¿›è¡Œæ ‡æ³¨..."):
                results = asyncio.run(run_batch_annotation(
                    target_df, 
                    st.session_state.system_prompt, 
                    st.session_state.user_prompt_template, 
                    schema, 
                    config,
                    progress_callback=update_progress
                ))
            
            status_text.success("âœ… æ ‡æ³¨ä»»åŠ¡å®Œæˆï¼")
            
            # Process Results
            results_map = {res["index"]: res for res in results}
            
            final_data = []
            for index, row in target_df.iterrows():
                row_data = row.to_dict()
                if index in results_map:
                    res = results_map[index]
                    if res["status"] == "success":
                        row_data.update(res["parsed"])
                        row_data["_raw_response"] = res["raw"]
                        row_data["_status"] = "success"
                    else:
                        row_data["_error"] = res.get("error", "Unknown Error")
                        row_data["_raw_response"] = res.get("raw", "")
                        row_data["_status"] = "error"
                else:
                    row_data["_status"] = "skipped"
                
                final_data.append(row_data)
            
            st.session_state.results_df = pd.DataFrame(final_data)
            
        except Exception as e:
            st.error(f"è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

    # --- 3. Results & Export ---
    if st.session_state.results_df is not None:
        st.markdown("---")
        st.subheader("3. ç»“æœä¸å¯¼å‡º")
        
        # Metrics
        df_res = st.session_state.results_df
        if "_status" in df_res.columns:
            success_count = len(df_res[df_res["_status"] == "success"])
            error_count = len(df_res[df_res["_status"] == "error"])
            rate = success_count / len(df_res) * 100
            
            c1, c2, c3 = st.columns(3)
            c1.metric("æˆåŠŸæ¡æ•°", success_count)
            c2.metric("å¤±è´¥æ¡æ•°", error_count)
            c3.metric("æˆåŠŸç‡", f"{rate:.1f}%")
        
        st.dataframe(df_res.head(100), width="stretch")
        if len(df_res) > 100:
            st.caption(f"ä»…å±•ç¤ºå‰ 100 è¡Œï¼Œå…± {len(df_res)} è¡Œã€‚è¯·ä¸‹è½½å®Œæ•´æ–‡ä»¶æŸ¥çœ‹ã€‚")

        # Download CSV
        csv = df_res.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ğŸ’¾ ä¸‹è½½æ ‡æ³¨ç»“æœ (CSV)",
            data=csv,
            file_name="annotated_results.csv",
            mime="text/csv",
            type="primary"
        )
        
        # --- Backup: Python Script ---
        with st.expander("ğŸ› ï¸ é™„åŠ åŠŸèƒ½ï¼šå¯¼å‡ºç¦»çº¿è¿è¡Œè„šæœ¬ (Python Script)"):
            st.info("å¦‚æœæ‚¨éœ€è¦åœ¨æœåŠ¡å™¨åå°è¿è¡Œæˆ–å¤„ç†è¶…å¤§æ•°æ®é›†ï¼Œå¯ä»¥å¯¼å‡ºæ­¤è„šæœ¬ã€‚")
            
            schema = convert_ui_fields_to_schema(st.session_state.schema_fields)
            script_content = generate_python_script(
                st.session_state.system_prompt, 
                st.session_state.user_prompt_template, 
                schema, 
                config
            )
            
            st.download_button(
                label="ä¸‹è½½ batch_label.py",
                data=script_content,
                file_name="batch_label.py",
                mime="text/x-python"
            )
